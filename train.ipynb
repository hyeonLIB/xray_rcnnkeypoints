{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OrthopedicLAB\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Tuple, List, Sequence, Callable, Dict  #  함수에 넘겨줄 파라미터들에 대해 타입들을 알려줌. 함수를 만들 때 사용자 입장에서 사용하기 좀 더 편하게 하는 라이브러리.\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn\n",
    "# from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Data Augmentation. 데이터 증대 시 필요한 라이브러리\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m'\u001b[39m\u001b[39m./data/images\u001b[39m\u001b[39m'\u001b[39m))):\n\u001b[0;32m      4\u001b[0m     image_id \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(\u001b[39m'\u001b[39m\u001b[39m./data/images\u001b[39m\u001b[39m'\u001b[39m)[index]\n\u001b[1;32m----> 5\u001b[0m     label_id \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m'\u001b[39;49m\u001b[39m./data/annotations\u001b[39;49m\u001b[39m'\u001b[39;49m)[index]\n\u001b[0;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m image_id\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m label_id\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      7\u001b[0m         count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Check if image file and label files label files in each directory has been sorted\n",
    "'''\n",
    "프로그램을 작성할 때 이미지와 레이블에 해당하는 두 가지 파일을 입력값으로 넣도록 작성했습니다.\n",
    "두 가지 파일은 각각 다른 폴더에 저장되어 있는데, 이 파일들은 폴더의 index 형식으로 불러들여집니다.\n",
    "따라서 각 폴더 내 데이터들의 순서가 정렬되어있지 않은 경우 이미지에 엉뚱한 정답지를 붙여 학습을 진행하는 경우가 발생합니다.\n",
    "ex) 1번째 이미지에 5번째 이미지의 레이블값이 정답지로 주어짐.\n",
    "이는 모델 학습에 치명적이므로 이미지 파일과 레이블 파일이 정렬되어있는지 확인하는 코드입니다.\n",
    "대부분은 정렬되어있습니다. (파일 이름을 맞춰 준 경우) -> Just in case\n",
    "'''\n",
    "count = 0\n",
    "for index in range(len(os.listdir('./data/images'))):\n",
    "    image_id = os.listdir('./data/images')[index]\n",
    "    label_id = os.listdir('./data/annotations')[index]\n",
    "    if image_id.replace('.jpg', '') == label_id.replace('.json', ''):\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path: os.PathLike,\n",
    "        label_path: os.PathLike,\n",
    "        transform: Sequence[Callable] = None,\n",
    "        demo:bool = False # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "    ) -> None:\n",
    "        self.image_path = image_path\n",
    "        self.label_path = label_path\n",
    "        self.transform = transform\n",
    "        self.demo = demo\n",
    "    '''\n",
    "    KeypointDataset 객체 : 학습에 필요한 입력 데이터를 반환하는 클래스 입니다.\n",
    "    이미지와 레이블의 경로를 읽어들여 외부에서 요청한 index 에 해당하는 이미지와 레이블을 각각 로드합니다.\n",
    "    각 이미지와 레이블은 학습 시 사용할 수 있는 tensor 데이터 형태로 변환 후 반환하게 됩니다.\n",
    "    target 구성 시 하나의 이미지 파일에 여러 target들을 만들어 원하는 부분을 학습하도록 할 수 있습니다.\n",
    "\n",
    "    def __getitem__ ()\n",
    "        output          :   image(tensor), targets(tensor) -> 한 장의 이미지만 출력함\n",
    "        input parameter -\n",
    "            image_path  :   os.PathLike \n",
    "            label_path  :   os.PathLike\n",
    "            transform   :   Sequence[Callable]  (default = None)\n",
    "            demo        :   bool                (default = False)\n",
    "            \n",
    "            image_path는 불러들일 이미지의 경로입니다.\n",
    "            label_path는 불러들일 레이블의 경로입니다.\n",
    "            transfrom은 이미지 증대 시 어떻게 이미지를 transform 해 줄지에 대한 정보를 갖고 있습니다.\n",
    "            demo는 transform 된 이미지와 되지 않은 원본 이미지를 반환합니다.\n",
    "                * 오로지 시각화 목적으로 사용합니다.\n",
    "                * 학습시에 demo 값을 True 로 둘 경우 -> 학습시엔 항상 demo = False\n",
    "                학습 입력 데이터는 한 번에 하나의 이미지와 레이블을 짝 지어서 입력받는 반면,\n",
    "                데이터가 transformed, original 각 이미지와 레이블이 구성되어 전달받기 때문에 에러를 발생시킵니다.\n",
    "    '''\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 이미지 파일이 저장된 경로내에 저장되어 있는 모든 파일의 숫자를 반환합니다.\n",
    "        return len(os.listdir(self.image_path))\n",
    "        \n",
    "    def __getitem__(self, index:int) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        image_id = os.listdir(self.image_path)[index]  #  폴더 내 index 번째 이미지 파일 읽기\n",
    "        label_id = os.listdir(self.label_path)[index]  #  폴더 내 index 번째 레이블 파일 읽기\n",
    "\n",
    "        with open(os.path.join(self.label_path, label_id)) as f:\n",
    "            label_data = json.load(f)\n",
    "            bboxes_original = label_data['bboxes']\n",
    "            bboxes_labels_original = [1,2]  # 0 is for background.\n",
    "            keypoints_original = label_data['keypoints']\n",
    "\n",
    "        img_original = cv2.imread(os.path.join(self.image_path, image_id), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.transform:   # augmentation process\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            \n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,2,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    # kp - coordinates of keypoint\n",
    "                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            print('transfrom doesn\"t work')\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # 모든 정보를 학습에 사용할 수 있는 tensor 형태로 데이터 타입을 변환합니다.        \n",
    "        # target들은 key value 값을 갖고 있는 dictionary 형태로 변환합니다.\n",
    "        # boxes      :  boundary 박스에 해당하는 꼭지점의 xy 좌표 정보를 갖고 있음\n",
    "        #               boundary 박스를 그리는 두 개의 꼭지점 왼쪽 상단, 오른쪽 하단 [ [x1, y1, x2, y2], [x1, y1, x2, y2] ]\n",
    "        # labels     :  박스에 대한 레이블. femoral 인지 tibial 인지 구분할 수 있도록\n",
    "        # area       :  boundary 박스의 면적\n",
    "        # keypoints  :  박스 안에서 관심있는 부분인 keypoints 들의 좌표\n",
    "        #               각 keypoints [ [ [ x1, y1] [ x2, y2 ] ], [ [ x1, y1] [ x2, y2] ] ] - 첫 번째는 femoral, 두 전째는 tibial   \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor([1,2], dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([index])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)        \n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1,2], dtype=torch.int64)\n",
    "        target_original[\"image_id\"] = torch.tensor([index])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo == True:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터 증대를 위해 이미지를 어떻게 transform 할 지 sequence를 정의해줍니다.\n",
    "데이터 증대 : 원본 이미지를 영상처리 기법을 통해 회전시키거나 밝기, 대비등을 조정하여 다른 형태의 이미지로 구성하는 기법\n",
    "             활용 시 하나의 이미지로 여러개의 이미지를 만들 수 있습니다.\n",
    "             이는 이미지 데이터가 부족할 때 학습 성능을 높이기 위해서 활용할 수 있으며, 과적합에 따른 성능 저하를 방지하는데 중요한 기법 중 하나입니다.\n",
    "'''\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    # 이미지를 회전시킴에 따라 keypoints와 boundary box도 회전되어야 합니다.\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )\n",
    "\n",
    "def collate_fn(batch: torch.Tensor) -> Tuple:\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original targets:\n",
      " ({'boxes': tensor([[3796., 1310., 4742., 5198.],\n",
      "        [3877., 4990., 4777., 8198.]]), 'labels': tensor([1, 2]), 'image_id': tensor([136]), 'area': tensor([3678048., 2887200.]), 'iscrowd': tensor([0, 0]), 'keypoints': tensor([[[4.5000e+03, 1.5430e+03, 1.0000e+00],\n",
      "         [4.3140e+03, 4.9890e+03, 1.0000e+00]],\n",
      "\n",
      "        [[4.3030e+03, 5.1520e+03, 1.0000e+00],\n",
      "         [4.4550e+03, 8.0100e+03, 1.0000e+00]]])},) \n",
      "\n",
      "\n",
      "Transformed targets:\n",
      " ({'boxes': tensor([[3802., 3796., 7690., 4742.],\n",
      "        [ 802., 3877., 4010., 4777.]]), 'labels': tensor([1, 2]), 'image_id': tensor([136]), 'area': tensor([3678048., 2887200.]), 'iscrowd': tensor([0, 0]), 'keypoints': tensor([[[7.4560e+03, 4.5000e+03, 1.0000e+00],\n",
      "         [4.0100e+03, 4.3140e+03, 1.0000e+00]],\n",
      "\n",
      "        [[3.8470e+03, 4.3030e+03, 1.0000e+00],\n",
      "         [9.8900e+02, 4.4550e+03, 1.0000e+00]]])},)\n"
     ]
    }
   ],
   "source": [
    "# 이미지와 레이블에 해당하는 경로를 지정하고 데이터들이 잘 구성되어 들어오는지 확인합니다.\n",
    "image_path = './data/images'\n",
    "label_path = './data/annotations'\n",
    "\n",
    "dataset = KeypointDataset(image_path, label_path, transform=train_transform(), demo=True) # demo for only visualization\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n",
    "print(\"Transformed targets:\\n\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, num_objects, weights_path=None):\n",
    "    '''\n",
    "    num_objects : int   |   예측하고자 하는 객체의 숫자를 지정합니다.(boundary box 숫자)\n",
    "                            * 여기서 이미지의 배경에 해당하는 객체도 포함시켜야 합니다.\n",
    "                            * 따라서 찾고자 하는 객체가 2개라면, 3을 입력시켜야 합니다.\n",
    "    num_keypoints : int |   예측하고자 하는 keypoints의 숫자를 지정합니다.\n",
    "    weight_path : str   |   학습된 모델의 weight 파일이 있을 경우, 저장된 weight 값을 불러옵니다.\n",
    "    '''\n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = keypointrcnn_resnet50_fpn(\n",
    "                                    # weights=False,            # Is deprecated since 0.13, default = None (Annotated cause it occurs warnings)\n",
    "                                    # weights_backbone=True,    # Is deprecated since 0.13, default = ResNet50_Weights.IMAGENET1K_V1 (Annotated cause it occurs warnings)\n",
    "                                    num_keypoints=num_keypoints,\n",
    "                                    num_classes=num_objects,    # Background is the first class, objects are other classes\n",
    "                                    rpn_anchor_generator=anchor_generator\n",
    "                                )\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, device='cuda:0'):\n",
    "    '''\n",
    "    원하는 학습 모델을 불러 학습을 진행합니다.\n",
    "    각각 optimizer와 epochs를 커스텀하여 학습을 진행합니다.\n",
    "    '''\n",
    "    model = get_model(num_keypoints=2, num_objects=3)\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "    num_epochs = 40\n",
    "    hist_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"number of epoch : \",epoch)\n",
    "        model.train()\n",
    "        for i, (images, targets) in enumerate(data_loader):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            optimizer.zero_grad()\n",
    "            losses = model(images, targets)\n",
    "    \n",
    "            loss = sum(loss for loss in losses.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f'| epoch: {epoch} | loss: {loss.item():.4f}', end=' | ')\n",
    "                for k, v in losses.items():\n",
    "                    print(f'{k[5:]}: {v.item():.4f}', end=' | ')\n",
    "                print()\n",
    "                hist_loss.append(round(loss.item(), 4))\n",
    "    \n",
    "    return model, hist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "number of epoch :  0\n",
      "| epoch: 0 | loss: 9.6470 | classifier: 0.7943 | box_reg: 0.0249 | keypoint: 8.1219 | objectness: 0.6946 | rpn_box_reg: 0.0113 | \n",
      "| epoch: 0 | loss: 9.1625 | classifier: 0.4059 | box_reg: 0.0631 | keypoint: 7.9837 | objectness: 0.6918 | rpn_box_reg: 0.0180 | \n",
      "| epoch: 0 | loss: 8.8122 | classifier: 0.2274 | box_reg: 0.0806 | keypoint: 7.8035 | objectness: 0.6877 | rpn_box_reg: 0.0130 | \n",
      "| epoch: 0 | loss: 8.3316 | classifier: 0.1395 | box_reg: 0.0442 | keypoint: 7.4511 | objectness: 0.6813 | rpn_box_reg: 0.0155 | \n",
      "| epoch: 0 | loss: 7.7973 | classifier: 0.1600 | box_reg: 0.0722 | keypoint: 6.8905 | objectness: 0.6594 | rpn_box_reg: 0.0152 | \n",
      "| epoch: 0 | loss: 7.4239 | classifier: 0.1504 | box_reg: 0.0606 | keypoint: 6.5909 | objectness: 0.6079 | rpn_box_reg: 0.0141 | \n",
      "number of epoch :  1\n",
      "| epoch: 1 | loss: 6.3217 | classifier: 0.1292 | box_reg: 0.0520 | keypoint: 5.6212 | objectness: 0.5079 | rpn_box_reg: 0.0114 | \n",
      "| epoch: 1 | loss: 6.6220 | classifier: 0.0926 | box_reg: 0.0620 | keypoint: 6.1005 | objectness: 0.3564 | rpn_box_reg: 0.0106 | \n",
      "| epoch: 1 | loss: 6.0003 | classifier: 0.0798 | box_reg: 0.0771 | keypoint: 5.6139 | objectness: 0.2121 | rpn_box_reg: 0.0174 | \n",
      "| epoch: 1 | loss: 5.6050 | classifier: 0.0832 | box_reg: 0.0989 | keypoint: 5.2820 | objectness: 0.1295 | rpn_box_reg: 0.0113 | \n",
      "| epoch: 1 | loss: 5.4883 | classifier: 0.0838 | box_reg: 0.1148 | keypoint: 5.1779 | objectness: 0.1010 | rpn_box_reg: 0.0109 | \n",
      "| epoch: 1 | loss: 5.7849 | classifier: 0.0827 | box_reg: 0.1397 | keypoint: 5.4801 | objectness: 0.0739 | rpn_box_reg: 0.0085 | \n",
      "number of epoch :  2\n",
      "| epoch: 2 | loss: 5.1859 | classifier: 0.0896 | box_reg: 0.1357 | keypoint: 4.8670 | objectness: 0.0795 | rpn_box_reg: 0.0141 | \n",
      "| epoch: 2 | loss: 5.1072 | classifier: 0.0688 | box_reg: 0.1275 | keypoint: 4.8297 | objectness: 0.0698 | rpn_box_reg: 0.0115 | \n",
      "| epoch: 2 | loss: 4.9552 | classifier: 0.0679 | box_reg: 0.1034 | keypoint: 4.7157 | objectness: 0.0590 | rpn_box_reg: 0.0092 | \n",
      "| epoch: 2 | loss: 4.7820 | classifier: 0.0876 | box_reg: 0.1425 | keypoint: 4.4942 | objectness: 0.0476 | rpn_box_reg: 0.0101 | \n",
      "| epoch: 2 | loss: 4.9226 | classifier: 0.0854 | box_reg: 0.1344 | keypoint: 4.6361 | objectness: 0.0538 | rpn_box_reg: 0.0130 | \n",
      "| epoch: 2 | loss: 4.9750 | classifier: 0.1156 | box_reg: 0.1449 | keypoint: 4.6436 | objectness: 0.0542 | rpn_box_reg: 0.0166 | \n",
      "number of epoch :  3\n",
      "| epoch: 3 | loss: 4.8662 | classifier: 0.0535 | box_reg: 0.1235 | keypoint: 4.6341 | objectness: 0.0447 | rpn_box_reg: 0.0104 | \n",
      "| epoch: 3 | loss: 4.5428 | classifier: 0.0518 | box_reg: 0.1148 | keypoint: 4.3220 | objectness: 0.0445 | rpn_box_reg: 0.0098 | \n",
      "| epoch: 3 | loss: 5.1222 | classifier: 0.0976 | box_reg: 0.1246 | keypoint: 4.8463 | objectness: 0.0430 | rpn_box_reg: 0.0108 | \n",
      "| epoch: 3 | loss: 4.8099 | classifier: 0.0779 | box_reg: 0.1719 | keypoint: 4.5056 | objectness: 0.0449 | rpn_box_reg: 0.0097 | \n",
      "| epoch: 3 | loss: 4.4732 | classifier: 0.0997 | box_reg: 0.1795 | keypoint: 4.1435 | objectness: 0.0393 | rpn_box_reg: 0.0113 | \n",
      "| epoch: 3 | loss: 4.2552 | classifier: 0.0912 | box_reg: 0.1460 | keypoint: 3.9707 | objectness: 0.0379 | rpn_box_reg: 0.0094 | \n",
      "number of epoch :  4\n",
      "| epoch: 4 | loss: 4.5392 | classifier: 0.0836 | box_reg: 0.1196 | keypoint: 4.2936 | objectness: 0.0350 | rpn_box_reg: 0.0074 | \n",
      "| epoch: 4 | loss: 4.4033 | classifier: 0.0756 | box_reg: 0.1451 | keypoint: 4.1303 | objectness: 0.0391 | rpn_box_reg: 0.0131 | \n",
      "| epoch: 4 | loss: 4.3552 | classifier: 0.0771 | box_reg: 0.1411 | keypoint: 4.0928 | objectness: 0.0335 | rpn_box_reg: 0.0107 | \n",
      "| epoch: 4 | loss: 4.1113 | classifier: 0.0852 | box_reg: 0.1225 | keypoint: 3.8614 | objectness: 0.0337 | rpn_box_reg: 0.0085 | \n",
      "| epoch: 4 | loss: 4.0792 | classifier: 0.0880 | box_reg: 0.1379 | keypoint: 3.8144 | objectness: 0.0291 | rpn_box_reg: 0.0097 | \n",
      "| epoch: 4 | loss: 4.6576 | classifier: 0.0576 | box_reg: 0.1443 | keypoint: 4.4149 | objectness: 0.0309 | rpn_box_reg: 0.0099 | \n",
      "number of epoch :  5\n",
      "| epoch: 5 | loss: 5.0855 | classifier: 0.0641 | box_reg: 0.1068 | keypoint: 4.8824 | objectness: 0.0262 | rpn_box_reg: 0.0060 | \n",
      "| epoch: 5 | loss: 4.5695 | classifier: 0.0651 | box_reg: 0.1111 | keypoint: 4.3417 | objectness: 0.0420 | rpn_box_reg: 0.0096 | \n",
      "| epoch: 5 | loss: 4.2228 | classifier: 0.0921 | box_reg: 0.1176 | keypoint: 3.9713 | objectness: 0.0327 | rpn_box_reg: 0.0091 | \n",
      "| epoch: 5 | loss: 4.0950 | classifier: 0.0796 | box_reg: 0.1366 | keypoint: 3.8440 | objectness: 0.0261 | rpn_box_reg: 0.0086 | \n",
      "| epoch: 5 | loss: 3.6625 | classifier: 0.0449 | box_reg: 0.1285 | keypoint: 3.4506 | objectness: 0.0317 | rpn_box_reg: 0.0068 | \n",
      "| epoch: 5 | loss: 3.7505 | classifier: 0.0588 | box_reg: 0.1196 | keypoint: 3.5387 | objectness: 0.0186 | rpn_box_reg: 0.0148 | \n",
      "number of epoch :  6\n",
      "| epoch: 6 | loss: 3.8236 | classifier: 0.0731 | box_reg: 0.1335 | keypoint: 3.5823 | objectness: 0.0248 | rpn_box_reg: 0.0098 | \n",
      "| epoch: 6 | loss: 3.6017 | classifier: 0.0746 | box_reg: 0.1159 | keypoint: 3.3818 | objectness: 0.0193 | rpn_box_reg: 0.0100 | \n",
      "| epoch: 6 | loss: 3.9516 | classifier: 0.0538 | box_reg: 0.0943 | keypoint: 3.7637 | objectness: 0.0288 | rpn_box_reg: 0.0109 | \n",
      "| epoch: 6 | loss: 3.9450 | classifier: 0.0694 | box_reg: 0.1128 | keypoint: 3.7328 | objectness: 0.0205 | rpn_box_reg: 0.0095 | \n",
      "| epoch: 6 | loss: 3.9217 | classifier: 0.0567 | box_reg: 0.1028 | keypoint: 3.7197 | objectness: 0.0309 | rpn_box_reg: 0.0115 | \n",
      "| epoch: 6 | loss: 3.5216 | classifier: 0.0688 | box_reg: 0.1027 | keypoint: 3.3219 | objectness: 0.0169 | rpn_box_reg: 0.0112 | \n",
      "number of epoch :  7\n",
      "| epoch: 7 | loss: 3.9646 | classifier: 0.0879 | box_reg: 0.1117 | keypoint: 3.7369 | objectness: 0.0199 | rpn_box_reg: 0.0082 | \n",
      "| epoch: 7 | loss: 3.8738 | classifier: 0.0665 | box_reg: 0.1130 | keypoint: 3.6634 | objectness: 0.0213 | rpn_box_reg: 0.0096 | \n",
      "| epoch: 7 | loss: 3.6571 | classifier: 0.0812 | box_reg: 0.1390 | keypoint: 3.4072 | objectness: 0.0192 | rpn_box_reg: 0.0105 | \n",
      "| epoch: 7 | loss: 3.5800 | classifier: 0.0634 | box_reg: 0.1119 | keypoint: 3.3752 | objectness: 0.0190 | rpn_box_reg: 0.0106 | \n",
      "| epoch: 7 | loss: 3.6879 | classifier: 0.0712 | box_reg: 0.1313 | keypoint: 3.4558 | objectness: 0.0186 | rpn_box_reg: 0.0110 | \n",
      "| epoch: 7 | loss: 4.3913 | classifier: 0.0639 | box_reg: 0.1173 | keypoint: 4.1863 | objectness: 0.0168 | rpn_box_reg: 0.0071 | \n",
      "number of epoch :  8\n",
      "| epoch: 8 | loss: 3.2859 | classifier: 0.0575 | box_reg: 0.0869 | keypoint: 3.1118 | objectness: 0.0199 | rpn_box_reg: 0.0098 | \n",
      "| epoch: 8 | loss: 3.5385 | classifier: 0.0530 | box_reg: 0.1083 | keypoint: 3.3518 | objectness: 0.0162 | rpn_box_reg: 0.0092 | \n",
      "| epoch: 8 | loss: 3.5763 | classifier: 0.0444 | box_reg: 0.0881 | keypoint: 3.4161 | objectness: 0.0179 | rpn_box_reg: 0.0098 | \n",
      "| epoch: 8 | loss: 3.4267 | classifier: 0.0643 | box_reg: 0.1023 | keypoint: 3.2308 | objectness: 0.0182 | rpn_box_reg: 0.0110 | \n",
      "| epoch: 8 | loss: 3.1467 | classifier: 0.0624 | box_reg: 0.1042 | keypoint: 2.9596 | objectness: 0.0124 | rpn_box_reg: 0.0082 | \n",
      "| epoch: 8 | loss: 3.1514 | classifier: 0.0697 | box_reg: 0.1171 | keypoint: 2.9377 | objectness: 0.0193 | rpn_box_reg: 0.0077 | \n",
      "number of epoch :  9\n",
      "| epoch: 9 | loss: 3.3982 | classifier: 0.0616 | box_reg: 0.1053 | keypoint: 3.2121 | objectness: 0.0120 | rpn_box_reg: 0.0072 | \n",
      "| epoch: 9 | loss: 3.4152 | classifier: 0.0597 | box_reg: 0.1091 | keypoint: 3.2214 | objectness: 0.0152 | rpn_box_reg: 0.0098 | \n",
      "| epoch: 9 | loss: 3.2621 | classifier: 0.0406 | box_reg: 0.0992 | keypoint: 3.0999 | objectness: 0.0148 | rpn_box_reg: 0.0077 | \n",
      "| epoch: 9 | loss: 3.2808 | classifier: 0.0602 | box_reg: 0.1052 | keypoint: 3.0944 | objectness: 0.0133 | rpn_box_reg: 0.0077 | \n",
      "| epoch: 9 | loss: 3.4191 | classifier: 0.0656 | box_reg: 0.1221 | keypoint: 3.2089 | objectness: 0.0151 | rpn_box_reg: 0.0074 | \n",
      "| epoch: 9 | loss: 3.3712 | classifier: 0.0599 | box_reg: 0.0930 | keypoint: 3.2044 | objectness: 0.0099 | rpn_box_reg: 0.0040 | \n",
      "number of epoch :  10\n",
      "| epoch: 10 | loss: 3.3732 | classifier: 0.0720 | box_reg: 0.1143 | keypoint: 3.1572 | objectness: 0.0179 | rpn_box_reg: 0.0118 | \n",
      "| epoch: 10 | loss: 3.1393 | classifier: 0.0649 | box_reg: 0.1171 | keypoint: 2.9378 | objectness: 0.0126 | rpn_box_reg: 0.0068 | \n",
      "| epoch: 10 | loss: 3.2553 | classifier: 0.0461 | box_reg: 0.0783 | keypoint: 3.1078 | objectness: 0.0159 | rpn_box_reg: 0.0072 | \n",
      "| epoch: 10 | loss: 3.1882 | classifier: 0.0538 | box_reg: 0.0946 | keypoint: 3.0161 | objectness: 0.0152 | rpn_box_reg: 0.0085 | \n",
      "| epoch: 10 | loss: 3.3255 | classifier: 0.0485 | box_reg: 0.0876 | keypoint: 3.1695 | objectness: 0.0128 | rpn_box_reg: 0.0071 | \n",
      "| epoch: 10 | loss: 3.1227 | classifier: 0.0539 | box_reg: 0.1000 | keypoint: 2.9479 | objectness: 0.0148 | rpn_box_reg: 0.0060 | \n",
      "number of epoch :  11\n",
      "| epoch: 11 | loss: 3.2451 | classifier: 0.0655 | box_reg: 0.1284 | keypoint: 3.0332 | objectness: 0.0119 | rpn_box_reg: 0.0061 | \n",
      "| epoch: 11 | loss: 3.6064 | classifier: 0.0608 | box_reg: 0.1160 | keypoint: 3.4008 | objectness: 0.0163 | rpn_box_reg: 0.0126 | \n",
      "| epoch: 11 | loss: 2.7322 | classifier: 0.0452 | box_reg: 0.0742 | keypoint: 2.5962 | objectness: 0.0098 | rpn_box_reg: 0.0069 | \n",
      "| epoch: 11 | loss: 3.0883 | classifier: 0.0482 | box_reg: 0.1054 | keypoint: 2.9177 | objectness: 0.0114 | rpn_box_reg: 0.0057 | \n",
      "| epoch: 11 | loss: 2.9715 | classifier: 0.0433 | box_reg: 0.0839 | keypoint: 2.8213 | objectness: 0.0164 | rpn_box_reg: 0.0066 | \n",
      "| epoch: 11 | loss: 3.1164 | classifier: 0.0690 | box_reg: 0.1189 | keypoint: 2.9044 | objectness: 0.0105 | rpn_box_reg: 0.0135 | \n",
      "number of epoch :  12\n",
      "| epoch: 12 | loss: 3.5062 | classifier: 0.0678 | box_reg: 0.1035 | keypoint: 3.3173 | objectness: 0.0098 | rpn_box_reg: 0.0079 | \n",
      "| epoch: 12 | loss: 3.4218 | classifier: 0.0397 | box_reg: 0.0848 | keypoint: 3.2802 | objectness: 0.0113 | rpn_box_reg: 0.0058 | \n",
      "| epoch: 12 | loss: 3.0740 | classifier: 0.0478 | box_reg: 0.0914 | keypoint: 2.9097 | objectness: 0.0177 | rpn_box_reg: 0.0073 | \n",
      "| epoch: 12 | loss: 3.2884 | classifier: 0.0539 | box_reg: 0.0939 | keypoint: 3.1247 | objectness: 0.0088 | rpn_box_reg: 0.0071 | \n",
      "| epoch: 12 | loss: 2.9990 | classifier: 0.0517 | box_reg: 0.0966 | keypoint: 2.8306 | objectness: 0.0142 | rpn_box_reg: 0.0059 | \n",
      "| epoch: 12 | loss: 3.2181 | classifier: 0.0553 | box_reg: 0.1012 | keypoint: 3.0488 | objectness: 0.0091 | rpn_box_reg: 0.0037 | \n",
      "number of epoch :  13\n",
      "| epoch: 13 | loss: 3.4061 | classifier: 0.0433 | box_reg: 0.0943 | keypoint: 3.2509 | objectness: 0.0099 | rpn_box_reg: 0.0077 | \n",
      "| epoch: 13 | loss: 3.0946 | classifier: 0.0533 | box_reg: 0.0977 | keypoint: 2.9240 | objectness: 0.0096 | rpn_box_reg: 0.0100 | \n",
      "| epoch: 13 | loss: 3.1508 | classifier: 0.0513 | box_reg: 0.1107 | keypoint: 2.9675 | objectness: 0.0085 | rpn_box_reg: 0.0128 | \n",
      "| epoch: 13 | loss: 2.9911 | classifier: 0.0428 | box_reg: 0.0913 | keypoint: 2.8418 | objectness: 0.0082 | rpn_box_reg: 0.0071 | \n",
      "| epoch: 13 | loss: 3.0669 | classifier: 0.0539 | box_reg: 0.1011 | keypoint: 2.8947 | objectness: 0.0063 | rpn_box_reg: 0.0109 | \n",
      "| epoch: 13 | loss: 3.4244 | classifier: 0.0398 | box_reg: 0.0986 | keypoint: 3.2702 | objectness: 0.0088 | rpn_box_reg: 0.0070 | \n",
      "number of epoch :  14\n",
      "| epoch: 14 | loss: 3.6597 | classifier: 0.0450 | box_reg: 0.1014 | keypoint: 3.4960 | objectness: 0.0130 | rpn_box_reg: 0.0043 | \n",
      "| epoch: 14 | loss: 2.9870 | classifier: 0.0581 | box_reg: 0.0902 | keypoint: 2.8230 | objectness: 0.0088 | rpn_box_reg: 0.0069 | \n",
      "| epoch: 14 | loss: 3.4812 | classifier: 0.0574 | box_reg: 0.0991 | keypoint: 3.3120 | objectness: 0.0060 | rpn_box_reg: 0.0067 | \n",
      "| epoch: 14 | loss: 3.5418 | classifier: 0.0436 | box_reg: 0.1228 | keypoint: 3.3534 | objectness: 0.0147 | rpn_box_reg: 0.0073 | \n",
      "| epoch: 14 | loss: 2.6018 | classifier: 0.0542 | box_reg: 0.0756 | keypoint: 2.4587 | objectness: 0.0081 | rpn_box_reg: 0.0051 | \n",
      "| epoch: 14 | loss: 3.3284 | classifier: 0.0683 | box_reg: 0.1247 | keypoint: 3.1183 | objectness: 0.0107 | rpn_box_reg: 0.0064 | \n",
      "number of epoch :  15\n",
      "| epoch: 15 | loss: 2.9612 | classifier: 0.0468 | box_reg: 0.0924 | keypoint: 2.8088 | objectness: 0.0067 | rpn_box_reg: 0.0065 | \n",
      "| epoch: 15 | loss: 2.9062 | classifier: 0.0764 | box_reg: 0.1088 | keypoint: 2.7046 | objectness: 0.0082 | rpn_box_reg: 0.0081 | \n",
      "| epoch: 15 | loss: 2.6900 | classifier: 0.0444 | box_reg: 0.1103 | keypoint: 2.5164 | objectness: 0.0109 | rpn_box_reg: 0.0080 | \n",
      "| epoch: 15 | loss: 2.4165 | classifier: 0.0423 | box_reg: 0.0853 | keypoint: 2.2736 | objectness: 0.0074 | rpn_box_reg: 0.0079 | \n",
      "| epoch: 15 | loss: 2.8687 | classifier: 0.0513 | box_reg: 0.0932 | keypoint: 2.7070 | objectness: 0.0100 | rpn_box_reg: 0.0072 | \n",
      "| epoch: 15 | loss: 3.0263 | classifier: 0.0399 | box_reg: 0.0766 | keypoint: 2.8907 | objectness: 0.0108 | rpn_box_reg: 0.0083 | \n",
      "number of epoch :  16\n",
      "| epoch: 16 | loss: 2.7190 | classifier: 0.0629 | box_reg: 0.1113 | keypoint: 2.5199 | objectness: 0.0179 | rpn_box_reg: 0.0070 | \n",
      "| epoch: 16 | loss: 2.4555 | classifier: 0.0488 | box_reg: 0.0938 | keypoint: 2.2922 | objectness: 0.0110 | rpn_box_reg: 0.0097 | \n",
      "| epoch: 16 | loss: 2.5467 | classifier: 0.0423 | box_reg: 0.1029 | keypoint: 2.3911 | objectness: 0.0052 | rpn_box_reg: 0.0053 | \n",
      "| epoch: 16 | loss: 2.9250 | classifier: 0.0349 | box_reg: 0.0909 | keypoint: 2.7785 | objectness: 0.0132 | rpn_box_reg: 0.0074 | \n",
      "| epoch: 16 | loss: 3.2777 | classifier: 0.0589 | box_reg: 0.1031 | keypoint: 3.1003 | objectness: 0.0077 | rpn_box_reg: 0.0077 | \n",
      "| epoch: 16 | loss: 2.7986 | classifier: 0.0509 | box_reg: 0.0756 | keypoint: 2.6568 | objectness: 0.0091 | rpn_box_reg: 0.0061 | \n",
      "number of epoch :  17\n",
      "| epoch: 17 | loss: 2.9269 | classifier: 0.0528 | box_reg: 0.0922 | keypoint: 2.7711 | objectness: 0.0064 | rpn_box_reg: 0.0045 | \n",
      "| epoch: 17 | loss: 2.7508 | classifier: 0.0463 | box_reg: 0.0984 | keypoint: 2.5901 | objectness: 0.0081 | rpn_box_reg: 0.0079 | \n",
      "| epoch: 17 | loss: 2.3084 | classifier: 0.0383 | box_reg: 0.0798 | keypoint: 2.1668 | objectness: 0.0139 | rpn_box_reg: 0.0097 | \n",
      "| epoch: 17 | loss: 2.6641 | classifier: 0.0332 | box_reg: 0.0802 | keypoint: 2.5333 | objectness: 0.0110 | rpn_box_reg: 0.0064 | \n",
      "| epoch: 17 | loss: 2.6858 | classifier: 0.0566 | box_reg: 0.0896 | keypoint: 2.5181 | objectness: 0.0107 | rpn_box_reg: 0.0109 | \n",
      "| epoch: 17 | loss: 2.8164 | classifier: 0.0472 | box_reg: 0.0851 | keypoint: 2.6753 | objectness: 0.0053 | rpn_box_reg: 0.0035 | \n",
      "number of epoch :  18\n",
      "| epoch: 18 | loss: 2.3339 | classifier: 0.0342 | box_reg: 0.0862 | keypoint: 2.2011 | objectness: 0.0082 | rpn_box_reg: 0.0042 | \n",
      "| epoch: 18 | loss: 2.5839 | classifier: 0.0444 | box_reg: 0.0793 | keypoint: 2.4489 | objectness: 0.0046 | rpn_box_reg: 0.0066 | \n",
      "| epoch: 18 | loss: 2.8640 | classifier: 0.0400 | box_reg: 0.0808 | keypoint: 2.7273 | objectness: 0.0053 | rpn_box_reg: 0.0107 | \n",
      "| epoch: 18 | loss: 2.8799 | classifier: 0.0461 | box_reg: 0.0841 | keypoint: 2.7386 | objectness: 0.0061 | rpn_box_reg: 0.0051 | \n",
      "| epoch: 18 | loss: 2.9964 | classifier: 0.0476 | box_reg: 0.0880 | keypoint: 2.8479 | objectness: 0.0094 | rpn_box_reg: 0.0035 | \n",
      "| epoch: 18 | loss: 2.5584 | classifier: 0.0463 | box_reg: 0.0772 | keypoint: 2.4181 | objectness: 0.0118 | rpn_box_reg: 0.0050 | \n",
      "number of epoch :  19\n",
      "| epoch: 19 | loss: 2.7634 | classifier: 0.0436 | box_reg: 0.0908 | keypoint: 2.6142 | objectness: 0.0098 | rpn_box_reg: 0.0050 | \n",
      "| epoch: 19 | loss: 2.4692 | classifier: 0.0529 | box_reg: 0.1054 | keypoint: 2.2918 | objectness: 0.0131 | rpn_box_reg: 0.0060 | \n",
      "| epoch: 19 | loss: 2.5343 | classifier: 0.0423 | box_reg: 0.0890 | keypoint: 2.3931 | objectness: 0.0035 | rpn_box_reg: 0.0064 | \n",
      "| epoch: 19 | loss: 2.6888 | classifier: 0.0649 | box_reg: 0.0863 | keypoint: 2.5237 | objectness: 0.0087 | rpn_box_reg: 0.0052 | \n",
      "| epoch: 19 | loss: 2.6753 | classifier: 0.0388 | box_reg: 0.0897 | keypoint: 2.5376 | objectness: 0.0048 | rpn_box_reg: 0.0045 | \n",
      "| epoch: 19 | loss: 2.3368 | classifier: 0.0361 | box_reg: 0.0809 | keypoint: 2.2031 | objectness: 0.0049 | rpn_box_reg: 0.0118 | \n",
      "number of epoch :  20\n",
      "| epoch: 20 | loss: 2.7411 | classifier: 0.0454 | box_reg: 0.0768 | keypoint: 2.6030 | objectness: 0.0099 | rpn_box_reg: 0.0061 | \n",
      "| epoch: 20 | loss: 2.4969 | classifier: 0.0456 | box_reg: 0.0873 | keypoint: 2.3408 | objectness: 0.0161 | rpn_box_reg: 0.0071 | \n",
      "| epoch: 20 | loss: 2.4246 | classifier: 0.0415 | box_reg: 0.0786 | keypoint: 2.2967 | objectness: 0.0041 | rpn_box_reg: 0.0037 | \n",
      "| epoch: 20 | loss: 3.2836 | classifier: 0.0499 | box_reg: 0.0988 | keypoint: 3.1210 | objectness: 0.0058 | rpn_box_reg: 0.0082 | \n",
      "| epoch: 20 | loss: 3.1073 | classifier: 0.0524 | box_reg: 0.0885 | keypoint: 2.9497 | objectness: 0.0093 | rpn_box_reg: 0.0074 | \n",
      "| epoch: 20 | loss: 2.7366 | classifier: 0.0424 | box_reg: 0.0835 | keypoint: 2.5997 | objectness: 0.0073 | rpn_box_reg: 0.0037 | \n",
      "number of epoch :  21\n",
      "| epoch: 21 | loss: 2.7417 | classifier: 0.0412 | box_reg: 0.0799 | keypoint: 2.6090 | objectness: 0.0052 | rpn_box_reg: 0.0065 | \n",
      "| epoch: 21 | loss: 2.8933 | classifier: 0.0416 | box_reg: 0.0814 | keypoint: 2.7512 | objectness: 0.0103 | rpn_box_reg: 0.0089 | \n",
      "| epoch: 21 | loss: 2.3434 | classifier: 0.0337 | box_reg: 0.0757 | keypoint: 2.2177 | objectness: 0.0062 | rpn_box_reg: 0.0100 | \n",
      "| epoch: 21 | loss: 2.6014 | classifier: 0.0421 | box_reg: 0.0672 | keypoint: 2.4828 | objectness: 0.0043 | rpn_box_reg: 0.0051 | \n",
      "| epoch: 21 | loss: 2.6140 | classifier: 0.0374 | box_reg: 0.0659 | keypoint: 2.5006 | objectness: 0.0039 | rpn_box_reg: 0.0061 | \n",
      "| epoch: 21 | loss: 3.3608 | classifier: 0.0494 | box_reg: 0.1263 | keypoint: 3.1709 | objectness: 0.0044 | rpn_box_reg: 0.0097 | \n",
      "number of epoch :  22\n",
      "| epoch: 22 | loss: 2.5776 | classifier: 0.0444 | box_reg: 0.0778 | keypoint: 2.4442 | objectness: 0.0042 | rpn_box_reg: 0.0071 | \n",
      "| epoch: 22 | loss: 2.7639 | classifier: 0.0323 | box_reg: 0.0789 | keypoint: 2.6403 | objectness: 0.0056 | rpn_box_reg: 0.0068 | \n",
      "| epoch: 22 | loss: 2.5855 | classifier: 0.0444 | box_reg: 0.0904 | keypoint: 2.4423 | objectness: 0.0047 | rpn_box_reg: 0.0038 | \n",
      "| epoch: 22 | loss: 2.3166 | classifier: 0.0376 | box_reg: 0.0735 | keypoint: 2.1979 | objectness: 0.0043 | rpn_box_reg: 0.0035 | \n",
      "| epoch: 22 | loss: 2.4705 | classifier: 0.0363 | box_reg: 0.0696 | keypoint: 2.3547 | objectness: 0.0043 | rpn_box_reg: 0.0056 | \n",
      "| epoch: 22 | loss: 2.3406 | classifier: 0.0435 | box_reg: 0.1026 | keypoint: 2.1723 | objectness: 0.0122 | rpn_box_reg: 0.0099 | \n",
      "number of epoch :  23\n",
      "| epoch: 23 | loss: 2.2076 | classifier: 0.0423 | box_reg: 0.0839 | keypoint: 2.0694 | objectness: 0.0048 | rpn_box_reg: 0.0072 | \n",
      "| epoch: 23 | loss: 2.4319 | classifier: 0.0450 | box_reg: 0.0693 | keypoint: 2.3064 | objectness: 0.0046 | rpn_box_reg: 0.0065 | \n",
      "| epoch: 23 | loss: 2.6365 | classifier: 0.0432 | box_reg: 0.0803 | keypoint: 2.5026 | objectness: 0.0041 | rpn_box_reg: 0.0063 | \n",
      "| epoch: 23 | loss: 2.9300 | classifier: 0.0295 | box_reg: 0.0585 | keypoint: 2.8300 | objectness: 0.0069 | rpn_box_reg: 0.0050 | \n",
      "| epoch: 23 | loss: 2.4205 | classifier: 0.0423 | box_reg: 0.0746 | keypoint: 2.2882 | objectness: 0.0081 | rpn_box_reg: 0.0073 | \n",
      "| epoch: 23 | loss: 2.7679 | classifier: 0.0290 | box_reg: 0.0678 | keypoint: 2.6516 | objectness: 0.0126 | rpn_box_reg: 0.0069 | \n",
      "number of epoch :  24\n",
      "| epoch: 24 | loss: 2.5588 | classifier: 0.0413 | box_reg: 0.0663 | keypoint: 2.4394 | objectness: 0.0058 | rpn_box_reg: 0.0060 | \n",
      "| epoch: 24 | loss: 2.3797 | classifier: 0.0371 | box_reg: 0.0782 | keypoint: 2.2535 | objectness: 0.0057 | rpn_box_reg: 0.0053 | \n",
      "| epoch: 24 | loss: 2.5510 | classifier: 0.0437 | box_reg: 0.0812 | keypoint: 2.4194 | objectness: 0.0032 | rpn_box_reg: 0.0034 | \n",
      "| epoch: 24 | loss: 2.2857 | classifier: 0.0349 | box_reg: 0.0709 | keypoint: 2.1704 | objectness: 0.0043 | rpn_box_reg: 0.0052 | \n",
      "| epoch: 24 | loss: 2.3877 | classifier: 0.0322 | box_reg: 0.0730 | keypoint: 2.2709 | objectness: 0.0064 | rpn_box_reg: 0.0053 | \n",
      "| epoch: 24 | loss: 2.2414 | classifier: 0.0263 | box_reg: 0.0774 | keypoint: 2.1276 | objectness: 0.0058 | rpn_box_reg: 0.0043 | \n",
      "number of epoch :  25\n",
      "| epoch: 25 | loss: 2.5245 | classifier: 0.0408 | box_reg: 0.0766 | keypoint: 2.4001 | objectness: 0.0028 | rpn_box_reg: 0.0042 | \n",
      "| epoch: 25 | loss: 2.8419 | classifier: 0.0495 | box_reg: 0.0838 | keypoint: 2.6996 | objectness: 0.0045 | rpn_box_reg: 0.0045 | \n",
      "| epoch: 25 | loss: 2.3438 | classifier: 0.0242 | box_reg: 0.0628 | keypoint: 2.2458 | objectness: 0.0069 | rpn_box_reg: 0.0041 | \n",
      "| epoch: 25 | loss: 2.3639 | classifier: 0.0334 | box_reg: 0.0850 | keypoint: 2.2340 | objectness: 0.0044 | rpn_box_reg: 0.0071 | \n",
      "| epoch: 25 | loss: 2.2970 | classifier: 0.0402 | box_reg: 0.0865 | keypoint: 2.1582 | objectness: 0.0062 | rpn_box_reg: 0.0059 | \n",
      "| epoch: 25 | loss: 2.0359 | classifier: 0.0376 | box_reg: 0.0737 | keypoint: 1.9096 | objectness: 0.0113 | rpn_box_reg: 0.0037 | \n",
      "number of epoch :  26\n",
      "| epoch: 26 | loss: 2.7780 | classifier: 0.0378 | box_reg: 0.0720 | keypoint: 2.6535 | objectness: 0.0091 | rpn_box_reg: 0.0056 | \n",
      "| epoch: 26 | loss: 2.5940 | classifier: 0.0435 | box_reg: 0.0772 | keypoint: 2.4564 | objectness: 0.0108 | rpn_box_reg: 0.0062 | \n",
      "| epoch: 26 | loss: 2.1724 | classifier: 0.0402 | box_reg: 0.0725 | keypoint: 2.0519 | objectness: 0.0032 | rpn_box_reg: 0.0046 | \n",
      "| epoch: 26 | loss: 2.3840 | classifier: 0.0453 | box_reg: 0.0788 | keypoint: 2.2513 | objectness: 0.0039 | rpn_box_reg: 0.0047 | \n",
      "| epoch: 26 | loss: 2.3865 | classifier: 0.0417 | box_reg: 0.0757 | keypoint: 2.2617 | objectness: 0.0034 | rpn_box_reg: 0.0040 | \n",
      "| epoch: 26 | loss: 2.1134 | classifier: 0.0405 | box_reg: 0.0805 | keypoint: 1.9778 | objectness: 0.0096 | rpn_box_reg: 0.0050 | \n",
      "number of epoch :  27\n",
      "| epoch: 27 | loss: 2.6032 | classifier: 0.0528 | box_reg: 0.0707 | keypoint: 2.4680 | objectness: 0.0051 | rpn_box_reg: 0.0066 | \n",
      "| epoch: 27 | loss: 2.8621 | classifier: 0.0374 | box_reg: 0.0668 | keypoint: 2.7465 | objectness: 0.0074 | rpn_box_reg: 0.0041 | \n",
      "| epoch: 27 | loss: 3.0865 | classifier: 0.0371 | box_reg: 0.0738 | keypoint: 2.9669 | objectness: 0.0048 | rpn_box_reg: 0.0039 | \n",
      "| epoch: 27 | loss: 2.6845 | classifier: 0.0373 | box_reg: 0.0559 | keypoint: 2.5798 | objectness: 0.0073 | rpn_box_reg: 0.0042 | \n",
      "| epoch: 27 | loss: 2.3811 | classifier: 0.0335 | box_reg: 0.0585 | keypoint: 2.2780 | objectness: 0.0056 | rpn_box_reg: 0.0054 | \n",
      "| epoch: 27 | loss: 2.9766 | classifier: 0.0479 | box_reg: 0.1009 | keypoint: 2.8215 | objectness: 0.0039 | rpn_box_reg: 0.0025 | \n",
      "number of epoch :  28\n",
      "| epoch: 28 | loss: 2.6028 | classifier: 0.0428 | box_reg: 0.0891 | keypoint: 2.4588 | objectness: 0.0065 | rpn_box_reg: 0.0056 | \n",
      "| epoch: 28 | loss: 2.4118 | classifier: 0.0302 | box_reg: 0.0574 | keypoint: 2.3118 | objectness: 0.0063 | rpn_box_reg: 0.0062 | \n",
      "| epoch: 28 | loss: 2.4564 | classifier: 0.0355 | box_reg: 0.0650 | keypoint: 2.3332 | objectness: 0.0150 | rpn_box_reg: 0.0076 | \n",
      "| epoch: 28 | loss: 2.5600 | classifier: 0.0429 | box_reg: 0.0740 | keypoint: 2.4340 | objectness: 0.0042 | rpn_box_reg: 0.0050 | \n",
      "| epoch: 28 | loss: 2.2001 | classifier: 0.0372 | box_reg: 0.0652 | keypoint: 2.0897 | objectness: 0.0028 | rpn_box_reg: 0.0050 | \n",
      "| epoch: 28 | loss: 2.1743 | classifier: 0.0426 | box_reg: 0.0449 | keypoint: 2.0780 | objectness: 0.0045 | rpn_box_reg: 0.0043 | \n",
      "number of epoch :  29\n",
      "| epoch: 29 | loss: 2.0357 | classifier: 0.0414 | box_reg: 0.0909 | keypoint: 1.8898 | objectness: 0.0080 | rpn_box_reg: 0.0056 | \n",
      "| epoch: 29 | loss: 2.6370 | classifier: 0.0445 | box_reg: 0.0667 | keypoint: 2.5162 | objectness: 0.0045 | rpn_box_reg: 0.0052 | \n",
      "| epoch: 29 | loss: 2.0781 | classifier: 0.0350 | box_reg: 0.0599 | keypoint: 1.9740 | objectness: 0.0045 | rpn_box_reg: 0.0048 | \n",
      "| epoch: 29 | loss: 1.9817 | classifier: 0.0354 | box_reg: 0.0664 | keypoint: 1.8722 | objectness: 0.0039 | rpn_box_reg: 0.0038 | \n",
      "| epoch: 29 | loss: 2.5244 | classifier: 0.0350 | box_reg: 0.0779 | keypoint: 2.3958 | objectness: 0.0089 | rpn_box_reg: 0.0067 | \n",
      "| epoch: 29 | loss: 2.1142 | classifier: 0.0339 | box_reg: 0.0644 | keypoint: 2.0072 | objectness: 0.0045 | rpn_box_reg: 0.0041 | \n",
      "number of epoch :  30\n",
      "| epoch: 30 | loss: 2.5814 | classifier: 0.0345 | box_reg: 0.0715 | keypoint: 2.4648 | objectness: 0.0064 | rpn_box_reg: 0.0041 | \n",
      "| epoch: 30 | loss: 2.0795 | classifier: 0.0220 | box_reg: 0.0649 | keypoint: 1.9805 | objectness: 0.0079 | rpn_box_reg: 0.0041 | \n",
      "| epoch: 30 | loss: 2.7362 | classifier: 0.0380 | box_reg: 0.0782 | keypoint: 2.6084 | objectness: 0.0058 | rpn_box_reg: 0.0059 | \n",
      "| epoch: 30 | loss: 2.2281 | classifier: 0.0372 | box_reg: 0.0603 | keypoint: 2.1140 | objectness: 0.0083 | rpn_box_reg: 0.0084 | \n",
      "| epoch: 30 | loss: 2.4310 | classifier: 0.0323 | box_reg: 0.0583 | keypoint: 2.3320 | objectness: 0.0035 | rpn_box_reg: 0.0050 | \n",
      "| epoch: 30 | loss: 2.3520 | classifier: 0.0374 | box_reg: 0.0647 | keypoint: 2.2400 | objectness: 0.0069 | rpn_box_reg: 0.0030 | \n",
      "number of epoch :  31\n",
      "| epoch: 31 | loss: 2.0547 | classifier: 0.0338 | box_reg: 0.0515 | keypoint: 1.9595 | objectness: 0.0065 | rpn_box_reg: 0.0035 | \n",
      "| epoch: 31 | loss: 2.3966 | classifier: 0.0382 | box_reg: 0.0730 | keypoint: 2.2750 | objectness: 0.0034 | rpn_box_reg: 0.0070 | \n",
      "| epoch: 31 | loss: 1.9625 | classifier: 0.0666 | box_reg: 0.0900 | keypoint: 1.7886 | objectness: 0.0123 | rpn_box_reg: 0.0050 | \n",
      "| epoch: 31 | loss: 2.4502 | classifier: 0.0344 | box_reg: 0.0768 | keypoint: 2.3277 | objectness: 0.0070 | rpn_box_reg: 0.0043 | \n",
      "| epoch: 31 | loss: 2.1421 | classifier: 0.0429 | box_reg: 0.0801 | keypoint: 2.0099 | objectness: 0.0043 | rpn_box_reg: 0.0048 | \n",
      "| epoch: 31 | loss: 2.5932 | classifier: 0.0350 | box_reg: 0.0841 | keypoint: 2.4675 | objectness: 0.0030 | rpn_box_reg: 0.0036 | \n",
      "number of epoch :  32\n",
      "| epoch: 32 | loss: 2.4823 | classifier: 0.0373 | box_reg: 0.0788 | keypoint: 2.3594 | objectness: 0.0028 | rpn_box_reg: 0.0040 | \n",
      "| epoch: 32 | loss: 2.7903 | classifier: 0.0475 | box_reg: 0.0854 | keypoint: 2.6455 | objectness: 0.0056 | rpn_box_reg: 0.0063 | \n",
      "| epoch: 32 | loss: 2.1198 | classifier: 0.0291 | box_reg: 0.0606 | keypoint: 2.0213 | objectness: 0.0041 | rpn_box_reg: 0.0047 | \n",
      "| epoch: 32 | loss: 2.3306 | classifier: 0.0342 | box_reg: 0.0681 | keypoint: 2.2056 | objectness: 0.0090 | rpn_box_reg: 0.0136 | \n",
      "| epoch: 32 | loss: 2.5551 | classifier: 0.0391 | box_reg: 0.0864 | keypoint: 2.4236 | objectness: 0.0024 | rpn_box_reg: 0.0036 | \n",
      "| epoch: 32 | loss: 2.4092 | classifier: 0.0361 | box_reg: 0.0674 | keypoint: 2.2955 | objectness: 0.0066 | rpn_box_reg: 0.0037 | \n",
      "number of epoch :  33\n",
      "| epoch: 33 | loss: 2.5062 | classifier: 0.0399 | box_reg: 0.0744 | keypoint: 2.3843 | objectness: 0.0039 | rpn_box_reg: 0.0038 | \n",
      "| epoch: 33 | loss: 2.5882 | classifier: 0.0319 | box_reg: 0.0678 | keypoint: 2.4828 | objectness: 0.0034 | rpn_box_reg: 0.0023 | \n",
      "| epoch: 33 | loss: 2.4678 | classifier: 0.0279 | box_reg: 0.0638 | keypoint: 2.3688 | objectness: 0.0032 | rpn_box_reg: 0.0041 | \n",
      "| epoch: 33 | loss: 2.1715 | classifier: 0.0364 | box_reg: 0.0726 | keypoint: 2.0539 | objectness: 0.0041 | rpn_box_reg: 0.0044 | \n",
      "| epoch: 33 | loss: 2.3447 | classifier: 0.0372 | box_reg: 0.0701 | keypoint: 2.2308 | objectness: 0.0028 | rpn_box_reg: 0.0038 | \n",
      "| epoch: 33 | loss: 2.5395 | classifier: 0.0557 | box_reg: 0.0696 | keypoint: 2.4088 | objectness: 0.0018 | rpn_box_reg: 0.0037 | \n",
      "number of epoch :  34\n",
      "| epoch: 34 | loss: 2.3480 | classifier: 0.0233 | box_reg: 0.0640 | keypoint: 2.2490 | objectness: 0.0040 | rpn_box_reg: 0.0078 | \n",
      "| epoch: 34 | loss: 2.6609 | classifier: 0.0431 | box_reg: 0.0683 | keypoint: 2.5399 | objectness: 0.0052 | rpn_box_reg: 0.0044 | \n",
      "| epoch: 34 | loss: 2.2990 | classifier: 0.0362 | box_reg: 0.0707 | keypoint: 2.1831 | objectness: 0.0060 | rpn_box_reg: 0.0030 | \n",
      "| epoch: 34 | loss: 2.0931 | classifier: 0.0280 | box_reg: 0.0638 | keypoint: 1.9938 | objectness: 0.0032 | rpn_box_reg: 0.0043 | \n",
      "| epoch: 34 | loss: 2.3494 | classifier: 0.0359 | box_reg: 0.0852 | keypoint: 2.2204 | objectness: 0.0021 | rpn_box_reg: 0.0058 | \n",
      "| epoch: 34 | loss: 1.9202 | classifier: 0.0180 | box_reg: 0.0497 | keypoint: 1.8481 | objectness: 0.0029 | rpn_box_reg: 0.0016 | \n",
      "number of epoch :  35\n",
      "| epoch: 35 | loss: 2.6908 | classifier: 0.0621 | box_reg: 0.0865 | keypoint: 2.5221 | objectness: 0.0141 | rpn_box_reg: 0.0060 | \n",
      "| epoch: 35 | loss: 2.1793 | classifier: 0.0247 | box_reg: 0.0640 | keypoint: 2.0838 | objectness: 0.0029 | rpn_box_reg: 0.0038 | \n",
      "| epoch: 35 | loss: 2.4555 | classifier: 0.0332 | box_reg: 0.0593 | keypoint: 2.3545 | objectness: 0.0049 | rpn_box_reg: 0.0036 | \n",
      "| epoch: 35 | loss: 2.0060 | classifier: 0.0301 | box_reg: 0.0539 | keypoint: 1.9159 | objectness: 0.0036 | rpn_box_reg: 0.0025 | \n",
      "| epoch: 35 | loss: 2.4849 | classifier: 0.0425 | box_reg: 0.0851 | keypoint: 2.3483 | objectness: 0.0046 | rpn_box_reg: 0.0044 | \n",
      "| epoch: 35 | loss: 2.0861 | classifier: 0.0438 | box_reg: 0.0886 | keypoint: 1.9427 | objectness: 0.0033 | rpn_box_reg: 0.0078 | \n",
      "number of epoch :  36\n",
      "| epoch: 36 | loss: 2.8950 | classifier: 0.0309 | box_reg: 0.0668 | keypoint: 2.7863 | objectness: 0.0079 | rpn_box_reg: 0.0030 | \n",
      "| epoch: 36 | loss: 2.3763 | classifier: 0.0412 | box_reg: 0.0718 | keypoint: 2.2562 | objectness: 0.0038 | rpn_box_reg: 0.0033 | \n",
      "| epoch: 36 | loss: 2.3948 | classifier: 0.0414 | box_reg: 0.0908 | keypoint: 2.2530 | objectness: 0.0066 | rpn_box_reg: 0.0030 | \n",
      "| epoch: 36 | loss: 2.4330 | classifier: 0.0318 | box_reg: 0.0727 | keypoint: 2.3214 | objectness: 0.0033 | rpn_box_reg: 0.0039 | \n",
      "| epoch: 36 | loss: 2.3589 | classifier: 0.0474 | box_reg: 0.0899 | keypoint: 2.2119 | objectness: 0.0056 | rpn_box_reg: 0.0041 | \n",
      "| epoch: 36 | loss: 2.3355 | classifier: 0.0558 | box_reg: 0.0803 | keypoint: 2.1888 | objectness: 0.0034 | rpn_box_reg: 0.0071 | \n",
      "number of epoch :  37\n",
      "| epoch: 37 | loss: 2.0624 | classifier: 0.0347 | box_reg: 0.0718 | keypoint: 1.9455 | objectness: 0.0058 | rpn_box_reg: 0.0047 | \n",
      "| epoch: 37 | loss: 2.1287 | classifier: 0.0354 | box_reg: 0.0717 | keypoint: 2.0121 | objectness: 0.0043 | rpn_box_reg: 0.0053 | \n",
      "| epoch: 37 | loss: 2.3541 | classifier: 0.0244 | box_reg: 0.0612 | keypoint: 2.2622 | objectness: 0.0022 | rpn_box_reg: 0.0042 | \n",
      "| epoch: 37 | loss: 2.0272 | classifier: 0.0371 | box_reg: 0.0732 | keypoint: 1.8799 | objectness: 0.0176 | rpn_box_reg: 0.0195 | \n",
      "| epoch: 37 | loss: 2.2684 | classifier: 0.0292 | box_reg: 0.0599 | keypoint: 2.1717 | objectness: 0.0043 | rpn_box_reg: 0.0034 | \n",
      "| epoch: 37 | loss: 2.3323 | classifier: 0.0398 | box_reg: 0.0578 | keypoint: 2.2294 | objectness: 0.0016 | rpn_box_reg: 0.0038 | \n",
      "number of epoch :  38\n",
      "| epoch: 38 | loss: 2.4555 | classifier: 0.0474 | box_reg: 0.0540 | keypoint: 2.3458 | objectness: 0.0045 | rpn_box_reg: 0.0038 | \n",
      "| epoch: 38 | loss: 1.9922 | classifier: 0.0353 | box_reg: 0.0526 | keypoint: 1.8972 | objectness: 0.0032 | rpn_box_reg: 0.0040 | \n",
      "| epoch: 38 | loss: 2.3643 | classifier: 0.0239 | box_reg: 0.0579 | keypoint: 2.2669 | objectness: 0.0077 | rpn_box_reg: 0.0079 | \n",
      "| epoch: 38 | loss: 2.2207 | classifier: 0.0395 | box_reg: 0.0665 | keypoint: 2.1025 | objectness: 0.0075 | rpn_box_reg: 0.0048 | \n",
      "| epoch: 38 | loss: 2.2362 | classifier: 0.0354 | box_reg: 0.0727 | keypoint: 2.1194 | objectness: 0.0047 | rpn_box_reg: 0.0040 | \n",
      "| epoch: 38 | loss: 2.3552 | classifier: 0.0299 | box_reg: 0.0626 | keypoint: 2.2554 | objectness: 0.0038 | rpn_box_reg: 0.0035 | \n",
      "number of epoch :  39\n",
      "| epoch: 39 | loss: 2.2846 | classifier: 0.0314 | box_reg: 0.0564 | keypoint: 2.1876 | objectness: 0.0045 | rpn_box_reg: 0.0047 | \n",
      "| epoch: 39 | loss: 2.4798 | classifier: 0.0329 | box_reg: 0.0678 | keypoint: 2.3664 | objectness: 0.0089 | rpn_box_reg: 0.0038 | \n",
      "| epoch: 39 | loss: 2.1365 | classifier: 0.0311 | box_reg: 0.0876 | keypoint: 2.0111 | objectness: 0.0027 | rpn_box_reg: 0.0040 | \n",
      "| epoch: 39 | loss: 1.9780 | classifier: 0.0303 | box_reg: 0.0567 | keypoint: 1.8815 | objectness: 0.0078 | rpn_box_reg: 0.0017 | \n",
      "| epoch: 39 | loss: 2.1273 | classifier: 0.0280 | box_reg: 0.0652 | keypoint: 2.0262 | objectness: 0.0037 | rpn_box_reg: 0.0042 | \n",
      "| epoch: 39 | loss: 2.4494 | classifier: 0.0305 | box_reg: 0.0553 | keypoint: 2.3581 | objectness: 0.0014 | rpn_box_reg: 0.0041 | \n",
      "number of epoch :  40\n",
      "| epoch: 40 | loss: 2.9683 | classifier: 0.0412 | box_reg: 0.0713 | keypoint: 2.8461 | objectness: 0.0051 | rpn_box_reg: 0.0046 | \n",
      "| epoch: 40 | loss: 2.2751 | classifier: 0.0317 | box_reg: 0.0586 | keypoint: 2.1778 | objectness: 0.0024 | rpn_box_reg: 0.0045 | \n",
      "| epoch: 40 | loss: 2.3143 | classifier: 0.0338 | box_reg: 0.0637 | keypoint: 2.2079 | objectness: 0.0049 | rpn_box_reg: 0.0040 | \n",
      "| epoch: 40 | loss: 2.1291 | classifier: 0.0390 | box_reg: 0.0743 | keypoint: 2.0061 | objectness: 0.0037 | rpn_box_reg: 0.0060 | \n",
      "| epoch: 40 | loss: 2.1145 | classifier: 0.0364 | box_reg: 0.0689 | keypoint: 2.0032 | objectness: 0.0037 | rpn_box_reg: 0.0023 | \n",
      "| epoch: 40 | loss: 2.0792 | classifier: 0.0342 | box_reg: 0.0600 | keypoint: 1.9784 | objectness: 0.0034 | rpn_box_reg: 0.0031 | \n",
      "number of epoch :  41\n",
      "| epoch: 41 | loss: 2.5696 | classifier: 0.0476 | box_reg: 0.0796 | keypoint: 2.4334 | objectness: 0.0036 | rpn_box_reg: 0.0055 | \n",
      "| epoch: 41 | loss: 2.3318 | classifier: 0.0330 | box_reg: 0.0599 | keypoint: 2.2322 | objectness: 0.0026 | rpn_box_reg: 0.0041 | \n",
      "| epoch: 41 | loss: 2.1144 | classifier: 0.0334 | box_reg: 0.0548 | keypoint: 2.0167 | objectness: 0.0051 | rpn_box_reg: 0.0044 | \n",
      "| epoch: 41 | loss: 2.2920 | classifier: 0.0298 | box_reg: 0.0553 | keypoint: 2.1952 | objectness: 0.0078 | rpn_box_reg: 0.0039 | \n",
      "| epoch: 41 | loss: 2.2602 | classifier: 0.0347 | box_reg: 0.0684 | keypoint: 2.1500 | objectness: 0.0027 | rpn_box_reg: 0.0043 | \n",
      "| epoch: 41 | loss: 2.9590 | classifier: 0.0369 | box_reg: 0.0808 | keypoint: 2.8364 | objectness: 0.0026 | rpn_box_reg: 0.0024 | \n",
      "number of epoch :  42\n",
      "| epoch: 42 | loss: 2.0423 | classifier: 0.0371 | box_reg: 0.0589 | keypoint: 1.9374 | objectness: 0.0038 | rpn_box_reg: 0.0050 | \n",
      "| epoch: 42 | loss: 2.2993 | classifier: 0.0332 | box_reg: 0.0647 | keypoint: 2.1943 | objectness: 0.0048 | rpn_box_reg: 0.0024 | \n",
      "| epoch: 42 | loss: 2.3940 | classifier: 0.0369 | box_reg: 0.0688 | keypoint: 2.2777 | objectness: 0.0061 | rpn_box_reg: 0.0045 | \n",
      "| epoch: 42 | loss: 2.4065 | classifier: 0.0429 | box_reg: 0.0601 | keypoint: 2.2906 | objectness: 0.0082 | rpn_box_reg: 0.0047 | \n",
      "| epoch: 42 | loss: 2.1715 | classifier: 0.0526 | box_reg: 0.0726 | keypoint: 2.0366 | objectness: 0.0049 | rpn_box_reg: 0.0049 | \n",
      "| epoch: 42 | loss: 1.7747 | classifier: 0.0295 | box_reg: 0.0667 | keypoint: 1.6720 | objectness: 0.0034 | rpn_box_reg: 0.0031 | \n",
      "number of epoch :  43\n",
      "| epoch: 43 | loss: 2.0075 | classifier: 0.0343 | box_reg: 0.0674 | keypoint: 1.9005 | objectness: 0.0012 | rpn_box_reg: 0.0043 | \n",
      "| epoch: 43 | loss: 2.2308 | classifier: 0.0391 | box_reg: 0.0764 | keypoint: 2.1047 | objectness: 0.0059 | rpn_box_reg: 0.0047 | \n",
      "| epoch: 43 | loss: 2.2966 | classifier: 0.0310 | box_reg: 0.0632 | keypoint: 2.1964 | objectness: 0.0025 | rpn_box_reg: 0.0035 | \n",
      "| epoch: 43 | loss: 2.1726 | classifier: 0.0299 | box_reg: 0.0574 | keypoint: 2.0797 | objectness: 0.0027 | rpn_box_reg: 0.0029 | \n",
      "| epoch: 43 | loss: 2.1008 | classifier: 0.0341 | box_reg: 0.0690 | keypoint: 1.9915 | objectness: 0.0015 | rpn_box_reg: 0.0047 | \n",
      "| epoch: 43 | loss: 2.3817 | classifier: 0.0345 | box_reg: 0.0525 | keypoint: 2.2879 | objectness: 0.0018 | rpn_box_reg: 0.0050 | \n",
      "number of epoch :  44\n",
      "| epoch: 44 | loss: 2.6435 | classifier: 0.0443 | box_reg: 0.0837 | keypoint: 2.5070 | objectness: 0.0046 | rpn_box_reg: 0.0040 | \n",
      "| epoch: 44 | loss: 1.8714 | classifier: 0.0315 | box_reg: 0.0621 | keypoint: 1.7714 | objectness: 0.0027 | rpn_box_reg: 0.0037 | \n",
      "| epoch: 44 | loss: 1.7645 | classifier: 0.0302 | box_reg: 0.0523 | keypoint: 1.6656 | objectness: 0.0105 | rpn_box_reg: 0.0059 | \n",
      "| epoch: 44 | loss: 2.3230 | classifier: 0.0388 | box_reg: 0.0611 | keypoint: 2.2136 | objectness: 0.0056 | rpn_box_reg: 0.0039 | \n",
      "| epoch: 44 | loss: 2.6891 | classifier: 0.0254 | box_reg: 0.0673 | keypoint: 2.5853 | objectness: 0.0054 | rpn_box_reg: 0.0057 | \n",
      "| epoch: 44 | loss: 2.4446 | classifier: 0.0428 | box_reg: 0.0856 | keypoint: 2.2927 | objectness: 0.0136 | rpn_box_reg: 0.0098 | \n",
      "number of epoch :  45\n",
      "| epoch: 45 | loss: 2.4161 | classifier: 0.0430 | box_reg: 0.0747 | keypoint: 2.2822 | objectness: 0.0113 | rpn_box_reg: 0.0048 | \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m dataset \u001b[39m=\u001b[39m KeypointDataset(image_path, label_path, transform\u001b[39m=\u001b[39mtrain_transform())\n\u001b[0;32m      5\u001b[0m data_loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mcollate_fn)\n\u001b[1;32m----> 7\u001b[0m trained_model, hist \u001b[39m=\u001b[39m train(data_loader\u001b[39m=\u001b[39;49mdata_loader, device\u001b[39m=\u001b[39;49mdevice)\n",
      "Cell \u001b[1;32mIn [7], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_loader, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnumber of epoch : \u001b[39m\u001b[39m\"\u001b[39m,epoch)\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfor\u001b[39;00m i, (images, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[0;32m     14\u001b[0m     images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     15\u001b[0m     targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n",
      "File \u001b[1;32mc:\\Users\\OrthopedicLAB\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\OrthopedicLAB\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\OrthopedicLAB\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\OrthopedicLAB\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn [3], line 19\u001b[0m, in \u001b[0;36mKeypointDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index:\u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Dict]:\n\u001b[0;32m     18\u001b[0m     image_id \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_path)[index]\n\u001b[1;32m---> 19\u001b[0m     label_id \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_path)[index]\n\u001b[0;32m     21\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_path, label_id)) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     22\u001b[0m         label_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# GPU 활용하여 학습 진행\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "dataset = KeypointDataset(image_path, label_path, transform=train_transform())\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "trained_model, hist = train(data_loader=data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./weights\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      2\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mxray_rcnnkeypoints_weight_exp04.pth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m torch\u001b[39m.\u001b[39msave(trained_model\u001b[39m.\u001b[39mstate_dict(), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, model_name))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "# 학습된 모델의 weight 값들을 저장합니다.\n",
    "model_path = './weights'\n",
    "model_name = 'xray_rcnnkeypoints_weight_exp04.pth'\n",
    "\n",
    "torch.save(trained_model.state_dict(), os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch가 잡아둔 GPU 메모리 제거\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(hist))), hist)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "# epoch 마다 학습이 진행된 상황(loss) 시각화\n",
    "plt.plot(list(range(len(hist))), hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a647068bef9d1b4c199dcb654ad446e7fe933e02b6df42141d51a9adbaca517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
